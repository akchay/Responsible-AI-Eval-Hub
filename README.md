✨ **Responsible AI Evaluation Hub** ✨

The future of AI is Responsible! This repository is a one-stop shop for curated, high-quality research on the Test and Evaluation of Responsible AI. 

<br/>

Responsible AI is a set of principles and practices that ensure AI is developed and used in a way that's beneficial, ethical, and minimizes risks. It's about building trust in AI by focusing on these core pillars:

1. Trustworthiness
2. Explainability
3. Fairness and Bias
4. Safety and Security
5. Privacy

<br/>

**1. Trustworthiness**

| Date  | Paper |
| ------------- | ------------- | 
| Apr 2024  | [Long-Form Factuality in Large Language Models](https://arxiv.org/pdf/2403.18802.pdf) | 

<br/>

**2. Explainability**

| Date  | Paper | 
| ------------- | ------------- | 
| Sep 2021 | [Counterfactual Evaluation for Explainable AI](https://arxiv.org/pdf/2109.01962.pdf) | 
| Aug 2021 | [What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors](https://dl.acm.org/doi/abs/10.1145/3447548.3467213) | 
| May 2021 | [Better Metrics for Evaluating Explainable Artificial Intelligence](https://dl.acm.org/doi/abs/10.5555/3463952.3463962) | 
| Jan 2021 | [How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations](https://arxiv.org/pdf/2101.08758.pdf) | 
| Dec 2020 | [How can I explain this to you? an empirical study of deep neural network explanation methods](https://dl.acm.org/doi/10.5555/3495724.3496078) 
| May 2020 | [Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://arxiv.org/abs/2005.01831) |
| Apr 2020 | [Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning](https://dl.acm.org/doi/10.1145/3313831.3376219) | 
| Jan 2020 | [Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making](https://dl.acm.org/doi/abs/10.1145/3351095.3372852) |  
| Jul 2019 | [Explainable AI in Industry](https://dl.acm.org/doi/10.1145/3292500.3332281) | 
| Mar 2019 | [The effects of example-based explanations in a machine learning interface](https://dl.acm.org/doi/10.1145/3301275.3302289) | 
| Feb 2018 | [Manipulating and Measuring Model Interpretability](https://arxiv.org/abs/1802.07810) | 
| Jan 2018 | [A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning](https://arxiv.org/abs/1801.05075) | 

<br/>

**3. Fairness and Bias**

| Date  | Paper | 
| ------------- | ------------- | 
| Jan 2024 | [Evaluating Gender Bias in Large Language Models via Chain-of-Thought Prompting](https://arxiv.org/abs/2401.15585) |
| Dec 2023 | [GPTBIAS: A Comprehensive Framework for Evaluating Bias in Large Language Models](https://arxiv.org/abs/2312.06315) |
| May 2022 | [You Reap What You Sow: On the Challenges of Bias Evaluation Under Multilingual Settings](https://aclanthology.org/2022.bigscience-1.3.pdf) |
| Jun 2021 | [RedditBias: A Real-World Resource for Bias Evaluation and Debiasing of Conversational Language Models](https://arxiv.org/abs/2106.03521) |
| Jun 2021 | [Towards Understanding and Mitigating Social Biases in Language Models](https://arxiv.org/abs/2106.13219) |
| Jun 2021 | [Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers](https://aclanthology.org/2021.naacl-main.189.pdf) |
| Apr 2020 | [StereoSet: Measuring stereotypical bias in pretrained language models](https://arxiv.org/abs/2004.09456) |
| Nov 2019 | [Reducing Sentiment Bias in Language Models via Counterfactual Evaluation](https://arxiv.org/abs/1911.03064) | 
| Apr 2019 | [Identifying and Reducing Gender Bias in Word-Level Language Models](https://arxiv.org/abs/1904.03035) |

<br/>

**4. Safety and Security**

<br/>
   
**5. Privacy**

<br/>
