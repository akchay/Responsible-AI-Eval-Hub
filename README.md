✨ **Responsible AI Evaluation Hub** ✨

The future of AI is Responsible! This repository is a one-stop shop for curated, high-quality research on the test and evaluation of responsible AI.

Responsible AI is a set of principles and practices that ensure AI is developed and used in a way that's beneficial, ethical, and minimizes risks. It's about building trust in AI by focusing on these core pillars:

1. Trustworthiness
2. Explainability
3. Safety and Security
4. Fairness and Bias
5. Privacy


**1. Trustworthiness** 

| Date  | Paper | Key Takeway |
| ------------- | ------------- | ------------- | 
| 3rd April 2024  | [Long-Form Factuality in Large Language Models](https://arxiv.org/pdf/2403.18802.pdf) | This paper proposes a novel evaluation method called Search-Augmented Factuality Evaluator (SAFE), which utilizes a search-enabled large language model to split a long-form response into individual facts, revise individual facts to be self-contained, determine the relevance of each individual fact to answering the prompt, and check the factuality of each relevant fact by issuing Google Search queries.


**2. Explainability**

| Date  | Paper | Key Takeway |
| ------------- | ------------- | ------------- | 
| 3rd April 2024  | [Long-Form Factuality in Large Language Models](https://arxiv.org/pdf/2403.18802.pdf) | This paper proposes a novel evaluation method called Search-Augmented Factuality Evaluator (SAFE), which utilizes a search-enabled large language model to split a long-form response into individual facts, revise individual facts to be self-contained, determine the relevance of each individual fact to answering the prompt, and check the factuality of each relevant fact by issuing Google Search queries.


**3. Safety and Security**

| Date  | Paper | Key Takeway |
| ------------- | ------------- | ------------- | 
| 3rd April 2024  | [Long-Form Factuality in Large Language Models](https://arxiv.org/pdf/2403.18802.pdf) | This paper proposes a novel evaluation method called Search-Augmented Factuality Evaluator (SAFE), which utilizes a search-enabled large language model to split a long-form response into individual facts, revise individual facts to be self-contained, determine the relevance of each individual fact to answering the prompt, and check the factuality of each relevant fact by issuing Google Search queries.


**4. Fairness and Bias**

| Date  | Paper | Key Takeway |
| ------------- | ------------- | ------------- | 
| 3rd April 2024  | [Long-Form Factuality in Large Language Models](https://arxiv.org/pdf/2403.18802.pdf) | This paper proposes a novel evaluation method called Search-Augmented Factuality Evaluator (SAFE), which utilizes a search-enabled large language model to split a long-form response into individual facts, revise individual facts to be self-contained, determine the relevance of each individual fact to answering the prompt, and check the factuality of each relevant fact by issuing Google Search queries.

**5. Privacy**

| Date  | Paper | Key Takeway |
| ------------- | ------------- | ------------- | 
| 3rd April 2024  | [Long-Form Factuality in Large Language Models](https://arxiv.org/pdf/2403.18802.pdf) | This paper proposes a novel evaluation method called Search-Augmented Factuality Evaluator (SAFE), which utilizes a search-enabled large language model to split a long-form response into individual facts, revise individual facts to be self-contained, determine the relevance of each individual fact to answering the prompt, and check the factuality of each relevant fact by issuing Google Search queries.
